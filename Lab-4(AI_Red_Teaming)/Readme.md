# Lab: AI Red Teaming Lab - Introduction

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-repo-path/AI-Red-Teaming-Lab.ipynb)

---

## ğŸ§  Overview

In this lab, you'll explore **Azure AI's Red Teaming tools** to evaluate how well your language models handle adversarial inputs and edge-case prompts. This is essential for ensuring the **robustness, safety, and reliability** of AI systems in real-world deployments.

Unlike basic evaluation tasks, red teaming simulates **malicious user behavior** and tests how your model responds under pressure using **multiple attack strategies** and **risky prompt categories**.

---

## ğŸ“˜ What You Will Learn

- âœ… How to configure a secure Foundry project for red teaming
- âœ… How to use Azure OpenAI with red team scanning
- âœ… How to simulate various attack strategies like ROT13, Unicode Confusables, and Base64
- âœ… How to test against risk categories like Violence, Hate/Unfairness, and Self-Harm
- âœ… How to interpret and use the output for model safety improvements

---

## ğŸ–¼ï¸ Output Preview

Hereâ€™s a preview of the type of dashboard outputs youâ€™ll get from the red teaming scan:

![Red Team Dashboard 1](images/img-2.png)

![Red Team Dashboard 2](images/img-1.png)


